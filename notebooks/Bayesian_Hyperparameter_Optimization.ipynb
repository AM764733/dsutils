{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian Hyperparameter Optimization",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "aHe_WZN6ZZYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bayesian Hyperparameter Optimization using Gaussian Processes\n",
        "\n",
        "TODO: intro\n",
        "\n",
        "Other packages for Bayesian optimization\n",
        "\n",
        "* [HyperOpt](http://hyperopt.github.io/hyperopt)\n",
        "* [skopt](https://scikit-optimize.github.io)\n",
        "* [HyperparameterHunter](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/README.md)\n",
        "* [Spearmint](https://github.com/HIPS/Spearmint)\n",
        "* [BayesOpt](https://github.com/rmcantin/bayesopt)\n",
        "* [SMAC](https://automl.github.io/SMAC3/stable)\n",
        "* [HPOlib2](https://automl.github.io/HPOlib2/stable)\n",
        "\n",
        "Also mention that tree parzen estimators have been used instead of gaussian processes"
      ]
    },
    {
      "metadata": {
        "id": "ePhYVIru2eJY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V4asotckVwF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Math\n",
        "\n",
        "TODO"
      ]
    },
    {
      "metadata": {
        "id": "LKIJJD5a207E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To determine what point to sample next, we can find the hyperparameters which maximize an \"acquisition function\".  The acquisition function is a function that when evaluated at $\\mathbf{x}$ (some combination of hyperparameters), tells us how advantagous it would be to evaluate our expensive function $f$ at that point.  That way, we can use the acquisition function to find the next combination of hyperparameters we should try!\n",
        "\n",
        "There are a few different functions which we could use for the acquisition function, including the probability of improvement, the expected improvement, and the upper confidence bound. Here we'll use the expected improvement as our acquisition function.\n",
        "\n",
        "The expected improvement for some combination of hyperparameters $\\mathbf{x}$ is, like the name suggests, the average improvement of that combination of hyperparameters over the best set we've found so far ($\\hat{\\mathbf{x}}$):\n",
        "\n",
        "$$\n",
        "EI(\\mathbf{x}) = \\mathbb{E}[\\max (0, ~ f(\\mathbf{x})-f(\\hat{\\mathbf{x}})) ]\n",
        "$$\n",
        "\n",
        "where $f(\\mathbf{x})$ is our Gaussian process' estimation as to our model's performance with hyperparameter combination $\\mathbf{x}$, and $f(\\hat{\\mathbf{x}})$ is the best performance that we've actually acheived so far.\n",
        "\n",
        "How do we actually compute that?  The Gaussian process is modeling the probability of our model's performance ($f$) as a function of the hyperparameters ($\\mathbf{x}$), and so at any value of $\\mathbf{x}$, it gives us a normal distribution for its prediction of the value of $f$:\n",
        "\n",
        "$$\n",
        "p(f(\\mathbf{x})|\\mathbf{x}) ~ \\sim ~ \\mathcal{N}(\\mu(\\mathbf{x}), \\sigma(\\mathbf{x}))\n",
        "$$\n",
        "\n",
        "So, we can integrate that probability distribution (times the improvement magnitude) above the value of our current best performance value.   \n",
        "\n",
        "TODO: diagram\n",
        "\n",
        "This gives us how much of an improvement we can expect with hyperparameters $\\mathbf{x}$ over our current best, $\\hat{\\mathbf{x}}$.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "EI(\\mathbf{x}) &= \\int_{f(\\hat{\\mathbf{x}})}^\\infty p(f(\\mathbf{x})|\\mathbf{x}) (f(\\mathbf{x})-f(\\hat{\\mathbf{x}}))  df(\\mathbf{x}) \\\\\n",
        "&= \\int_{f(\\hat{\\mathbf{x}})}^\\infty\\mathcal{N}(f(\\mathbf{x})|\\mu(\\mathbf{x}), \\sigma(\\mathbf{x})) ~ (f(\\mathbf{x})-f(\\hat{\\mathbf{x}}))  ~ df(\\mathbf{x}) \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "After \"applying some tedious integration by parts\" ([Jones et al., 1998](https://link.springer.com/article/10.1023/A:1008306431147)),  we have an analytical solution:\n",
        "\n",
        "$$\n",
        "EI(x) = (\\mu(\\mathbf{x})-f(\\hat{\\mathbf{x}}))\\Phi(z) + \\sigma(\\mathbf{x}) \\phi(z)\n",
        "$$\n",
        "\n",
        "where $\\Phi$ is the cumulative distribution function of the standard normal distribution, $\\phi$ is the probability density function of the standard normal, and $z$ is the scaled difference between the Gaussian process mean and our current best performance:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu(\\mathbf{x})-f(\\hat{\\mathbf{x}})}{\\sigma(\\mathbf{x})}\n",
        "$$\n",
        "\n",
        "\n",
        "References:\n",
        "A good tutorial here:\n",
        "[Brochu et al., 2010](https://arxiv.org/abs/1012.2599))"
      ]
    },
    {
      "metadata": {
        "id": "oh39N8sN2ro9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization\n",
        "\n",
        "TODO: build a class for optimization w/ a gaussian process, function by function"
      ]
    },
    {
      "metadata": {
        "id": "HAtlOMba2w6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "class GaussianProcessOptimizer():\n",
        "    \"\"\"Bayesian function optimizer which uses a Gaussian process to model the\n",
        "    expensive function, and expected improvement as the acquisition function.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lb, ub,\n",
        "                 dtype=None,\n",
        "                 param_names=None,\n",
        "                 minimize=True,\n",
        "                 kernel=RationalQuadratic()+WhiteKernel(noise_level=1e-4),\n",
        "                 n_restarts_optimizer=10):\n",
        "        \"\"\"Gaussian process-based optimizer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lb : list\n",
        "            Lower bound for each parameter.\n",
        "        ub : list\n",
        "            Upper bound for each parameter.\n",
        "        dtype : list\n",
        "            Datatype for each parameter.  int or float\n",
        "            Default is to assume float for all parameters.\n",
        "        param_names : list of str\n",
        "            Parameter names\n",
        "        minimize : bool\n",
        "            Whether to minimize (True) or maximize (False).\n",
        "        kernel : sklearn.gaussian_process.kernels.Kernel\n",
        "            Kernel for the Gaussian process.\n",
        "            Default = RationalQuadratic + WhiteKernel with a noise_level\n",
        "            of 1e-4, which allows us to also model the noise.\n",
        "        n_restarts_optimizer : int >= 0\n",
        "            Number of times to re-seed the GP optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        # If passed info for a single parameter, don't require list\n",
        "        if isinstance(lb, (int, float)) and isinstance(ub, (int, float)):\n",
        "            lb = [lb]\n",
        "            ub = [ub]\n",
        "        if dtype is None or dtype is int or dtype is float:\n",
        "            dtype = [dtype]\n",
        "        if param_names is None or param_names is str:\n",
        "            param_names = [param_names]\n",
        "\n",
        "        # Check types\n",
        "        if not isinstance(lb, list) or not isinstance(ub, list):\n",
        "            raise TypeError('lb and ub must be lists')\n",
        "        if not isinstance(dtype, list):\n",
        "            raise TypeError('dtype must be a list')\n",
        "        if len(lb) != len(ub) or len(ub) != len(dtype):\n",
        "            raise ValueError('lb, ub, and dtype must be same length')\n",
        "        if not isinstance(minimize, bool):\n",
        "            raise TypeError('minimize must be True or False')\n",
        "        if not isinstance(kernel, Kernel):\n",
        "            raise TypeError('kernel must be an sklearn GP kernel')\n",
        "        if not isinstance(n_restarts_optimizer, int):\n",
        "            raise TypeError('n_restarts_optimizer must be an int')\n",
        "        if n_restarts_optimizer < 0:\n",
        "            raise ValueError('n_restarts_optimizer must be non-negative')\n",
        "\n",
        "        # Assume float if dtype not specified\n",
        "        self.num_dims = len(lb)\n",
        "        if dtype[0] is None:\n",
        "            dtype = [float]*self.num_dims\n",
        "\n",
        "        # Assign parameter names if not specified\n",
        "        if param_names[0] is None:\n",
        "            param_names = [None]*self.num_dims\n",
        "            for iP in range(self.num_dims):\n",
        "                param_names[iP] = 'Parameter_'+str(iP)\n",
        "\n",
        "        # Store parameters\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.db = [ub[i]-lb[i] for i in range(self.num_dims)]\n",
        "        self.bounds = [(lb[i], ub[i]) for i in range(self.num_dims)]\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.dtype = dtype\n",
        "        self.param_names = param_names\n",
        "        self.minimize = minimize\n",
        "        self.gp = GaussianProcessRegressor(\n",
        "            kernel=kernel, alpha=0.0,\n",
        "            n_restarts_optimizer=n_restarts_optimizer,\n",
        "        )\n",
        "\n",
        "        # Keep track of highest (or lowest) point so far\n",
        "        if minimize:\n",
        "            self.opt_y = np.inf\n",
        "            self.opt_x = None\n",
        "        else:\n",
        "            self.opt_y = -np.inf\n",
        "            self.opt_x = None\n",
        "\n",
        "\n",
        "    def _ensure_types(self, x):\n",
        "        \"\"\"Ensure types in x match dtype.\"\"\"\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = x.tolist()\n",
        "        for iP in range(self.num_dims):\n",
        "            if self.dtype[iP] is int:\n",
        "                x[iP] = round(x[iP])\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _make_dict(self, x):\n",
        "        \"\"\"Make x a dict w/ keys=dimension names\"\"\"\n",
        "        xo = dict()\n",
        "        for iP in range(self.num_dims):\n",
        "            xo[self.param_names[iP]] = x[iP]\n",
        "        return xo\n",
        "\n",
        "\n",
        "    def _fit_gp(self, step=None):\n",
        "        \"\"\"Fit the Gaussian process to data\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Set step if not specified\n",
        "        if step is None:\n",
        "            step = len(self.x)\n",
        "\n",
        "        # Fit GP to 1st ``step`` steps\n",
        "        x = self.x[:step]\n",
        "        y = self.y[:step]\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        x = np.array(x).astype('float64')\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Normalize y\n",
        "        self._y_mean = y.mean()\n",
        "        self._y_std = y.std()\n",
        "        y = (y-self._y_mean)/self._y_std\n",
        "\n",
        "        # Normalize x\n",
        "        for iD in range(self.num_dims):\n",
        "            x[:, iD] = (x[:, iD] - self.lb[iD]) / self.db[iD]\n",
        "\n",
        "        # Add jitter to x\n",
        "        x += 1e-5*np.random.standard_normal(x.shape)\n",
        "\n",
        "        # Fit the Gaussian process\n",
        "        self.gp = self.gp.fit(x, y)\n",
        "\n",
        "\n",
        "    def _pred_gp(self, x, return_std=False):\n",
        "        \"\"\"Predict y with the Gaussian process.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert to numpy array\n",
        "        x = np.array(x).astype('float64')\n",
        "\n",
        "        # Normalize x\n",
        "        for iD in range(self.num_dims):\n",
        "            x[:, iD] = (x[:, iD] - self.lb[iD]) / self.db[iD]\n",
        "\n",
        "        # Add jitter to x\n",
        "        x = x + 1e-5*np.random.standard_normal(x.shape)\n",
        "\n",
        "        # Predict y\n",
        "        y, y_std = self.gp.predict(x, return_std=True)\n",
        "\n",
        "        # Convert back to true scale\n",
        "        y = y*self._y_std+self._y_mean\n",
        "        y_std = y_std*self._y_std\n",
        "\n",
        "        # Return std dev if requested\n",
        "        if return_std:\n",
        "            return y, y_std\n",
        "        else:\n",
        "            return y\n",
        "\n",
        "\n",
        "    def add_point(self, x, y):\n",
        "        \"\"\"Add a point to the history of sampled points.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : list or int or float\n",
        "            The point's coordinates.\n",
        "        y : list or float\n",
        "            Function value(s) at this point.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert to list if passed single values\n",
        "        if isinstance(x, (int, float)):\n",
        "            x = [x]\n",
        "        if isinstance(y, float):\n",
        "            y = [y]\n",
        "\n",
        "        # Check inputs\n",
        "        if not all(isinstance(e, (int, float)) for e in x):\n",
        "            raise TypeError('x must be a list of ints or floats')\n",
        "        if not isinstance(y, (float, list, np.ndarray)):\n",
        "            raise TypeError('y must be a float, list, or ndarray')\n",
        "        if isinstance(y, np.ndarray):\n",
        "            y = y.tolist()\n",
        "        if isinstance(y, list):\n",
        "            if not all(isinstance(e, float) for e in y):\n",
        "                raise TypeError('y must be a list of floats')\n",
        "        if len(x) != self.num_dims:\n",
        "            raise RuntimeError('x has incorrect length')\n",
        "\n",
        "        # Append to sample record\n",
        "        if isinstance(y, list): #repeated x values\n",
        "            for ty in y:\n",
        "                self.x.append(x)\n",
        "                self.y.append(ty)\n",
        "            ty = np.array(y).mean()\n",
        "        else:\n",
        "            self.x.append(x)\n",
        "            self.y.append(y)\n",
        "            ty = y\n",
        "\n",
        "        # Store best point so far\n",
        "        if self.minimize:\n",
        "            if ty < self.opt_y:\n",
        "                self.opt_y = ty\n",
        "                self.opt_x = x\n",
        "        else:\n",
        "            if ty > self.opt_y:\n",
        "                self.opt_y = ty\n",
        "                self.opt_x = x\n",
        "\n",
        "\n",
        "    def random_point(self, get_dict=False):\n",
        "        \"\"\"Get a random point within the bounds.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        get_dict : bool\n",
        "            Whether to return a dict w/ keys=dimension names (True), or just\n",
        "            a list (False, the default).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list or dict\n",
        "            List of parameter values, or a dict if dict=True\n",
        "        \"\"\"\n",
        "        x = np.random.uniform(self.lb, self.ub)\n",
        "        x = self._ensure_types(x)\n",
        "        if get_dict: \n",
        "            x = self._make_dict(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _expected_improvement(self, x):\n",
        "        \"\"\"Compute the expected improvement at x.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : ndarray\n",
        "            Point at which to evaluate the expected improvement\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The expected improvement at x\n",
        "        \"\"\"\n",
        "\n",
        "        # Predict performance at x\n",
        "        mu, sigma = self._pred_gp(x.reshape(-1, self.num_dims),\n",
        "                                  return_std=True)\n",
        "\n",
        "        # Compute and return expected improvement\n",
        "        flip = np.power(-1, self.minimize)\n",
        "        z = flip*(mu-self.opt_y)/sigma\n",
        "        return flip*(mu-self.opt_y)*norm.cdf(z) + sigma*norm.pdf(z)\n",
        "\n",
        "\n",
        "    def next_point(self, get_dict=False, n_restarts=10):\n",
        "        \"\"\"Get the point with the highest expected improvement.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        get_dict : bool\n",
        "            Whether to return a dict w/ keys=dimension names (True), or just\n",
        "            a list (False, the default).\n",
        "        n_restarts : int\n",
        "            Number of times to restart the optimizer.\n",
        "            Default = 10\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list or dict\n",
        "            List of parameter values for the next suggested point to sample,\n",
        "            or a dict if dict=True\n",
        "        \"\"\"\n",
        "\n",
        "        # Fit the Gaussian process to samples so far\n",
        "        self._fit_gp()\n",
        "\n",
        "        # Find x with greatest expected improvement\n",
        "        x = self.random_point()\n",
        "        best_score = np.inf\n",
        "        for iR in range(n_restarts):\n",
        "\n",
        "            # Maximize expected improvement\n",
        "            res = minimize(lambda x: -self._expected_improvement(x),\n",
        "                           self.random_point(),\n",
        "                           method='L-BFGS-B',\n",
        "                           bounds=self.bounds)\n",
        "\n",
        "            # Keep x if it's the best so far\n",
        "            if res.fun < best_score:\n",
        "                best_score = res.fun\n",
        "                x = res.x\n",
        "\n",
        "        # Return x with highest expected improvement\n",
        "        x = self._ensure_types(x)\n",
        "        if get_dict: \n",
        "            x = self._make_dict(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def best_point(self, expected=True, get_dict=False, n_restarts=10):\n",
        "        \"\"\"Get the expected best point.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        expected : bool\n",
        "            Whether to return the expected best point based on the fit \n",
        "            Gaussian process (True, the default), or to return the best\n",
        "            point which was actually sampled so far (False).\n",
        "        get_dict : bool\n",
        "            Whether to return a dict w/ keys=dimension names (True), or just\n",
        "            a list (False, the default).\n",
        "        n_restarts : int\n",
        "            Number of times to restart the optimizer.\n",
        "            Default = 10\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list or dict\n",
        "            List of x values for the best expected point,\n",
        "            or a dict if dict=True\n",
        "        \"\"\"\n",
        "\n",
        "        # Fit the Gaussian process to samples so far\n",
        "        self._fit_gp()\n",
        "\n",
        "        # Find x with greatest expected score\n",
        "        flip = np.power(-1, self.minimize)\n",
        "        score_func = lambda x: flip*self._pred_gp(x.reshape(-1,self.num_dims))\n",
        "        x = self.random_point()\n",
        "        best_score = np.inf\n",
        "        for iR in range(n_restarts):\n",
        "\n",
        "            # Maximize expected improvement\n",
        "            res = minimize(score_func, \n",
        "                           self.random_point(),\n",
        "                           method='L-BFGS-B',\n",
        "                           bounds=self.bounds)\n",
        "\n",
        "            # Keep x if it's the best so far\n",
        "            if res.fun < best_score:\n",
        "                best_score = res.fun\n",
        "                x = res.x\n",
        "\n",
        "        # Return x with highest expected improvement\n",
        "        x = self._ensure_types(x)\n",
        "        if get_dict: \n",
        "            x = self._make_dict(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_x(self, get_dict=False):\n",
        "        \"\"\"Get the sampled x values.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        get_dict : bool\n",
        "            Whether to return a dict w/ keys=dimension names (True), or just\n",
        "            a list (False, the default).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of lists or dict of lists\n",
        "            List sampled x values.  Each element of the list is a list with \n",
        "            the x values for that sample.\n",
        "        \"\"\"\n",
        "        if get_dict:\n",
        "            xo = dict()\n",
        "            for iP in range(self.num_dims):\n",
        "                xo[self.param_names[iP]] = [s[iP] for s in self.x]\n",
        "            return xo\n",
        "        else:\n",
        "            return self.x\n",
        "\n",
        "\n",
        "    def get_y(self):\n",
        "        \"\"\"Get the sampled y values.\"\"\"\n",
        "        return self.y\n",
        "\n",
        "\n",
        "    def plot_surface(self, x_dim=None, y_dim=None, res=100, step=None,\n",
        "                     refit=True):\n",
        "        \"\"\"Plot the estimated surface of the function being evaluated.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_dim : None, str, or int\n",
        "            Name or index of parameter to plot on the x axis.\n",
        "            If none specified, uses the first parameter.\n",
        "        y_dim : None, str, or int\n",
        "            Name or index of parameter to plot on the y axis.\n",
        "            If none specified, plots only a 1D plot of x_dim\n",
        "        res : int > 1\n",
        "            Resolution of the plot\n",
        "        step : None or int\n",
        "            Plot loss surface only from points up to step step.\n",
        "        refit : bool\n",
        "            Whether to re-fit the Gaussian process.\n",
        "            Default = True\n",
        "        \"\"\"\n",
        "\n",
        "        # Check inputs\n",
        "        if x_dim is not None and not isinstance(x_dim, (int, str)):\n",
        "            raise TypeError('x_dim must be None, str, or int')\n",
        "        if y_dim is not None and not isinstance(y_dim, (int, str)):\n",
        "            raise TypeError('y_dim must be None, str, or int')\n",
        "        if not isinstance(res, int):\n",
        "            raise TypeError('res must be an int')\n",
        "        if res < 1:\n",
        "            raise ValueError('res must be positive')\n",
        "        if step is not None and not isinstance(step, int):\n",
        "            raise TypeError('step must be None or an int')\n",
        "        if isinstance(step, int) and step < 0:\n",
        "            raise ValueError('step must be non-negative')\n",
        "\n",
        "        # Convert x_dim and y_dim to int if they are strings\n",
        "        if isinstance(x_dim, str):\n",
        "            x_dim = self.param_names.index(x_dim)\n",
        "        if isinstance(y_dim, str):\n",
        "            y_dim = self.param_names.index(y_dim)\n",
        "\n",
        "        # Set x_dim if not specified\n",
        "        if x_dim is None:\n",
        "            x_dim = 0\n",
        "\n",
        "        # Fit GP to 1st ``step`` steps\n",
        "        if refit:\n",
        "            self._fit_gp(step=step)\n",
        "\n",
        "        # 1D plot\n",
        "        if y_dim is None:\n",
        "\n",
        "            # Predict y as a fn of x (other params being @ middle of bounds)\n",
        "            x_pred = np.ones((res, self.num_dims))\n",
        "            x_pred *= (np.array(self.bounds)\n",
        "                       .mean(axis=1)\n",
        "                       .reshape(-1, self.num_dims))\n",
        "            x_pred[:,x_dim] = np.linspace(self.lb[x_dim], self.ub[x_dim], res)\n",
        "            y_pred, y_err = self._pred_gp(x_pred, return_std=True)\n",
        "\n",
        "            # Plot the Gaussian process' estimate of the function\n",
        "            plot_err(x_pred[:, x_dim], y_pred, y_err)\n",
        "            plt.xlabel(self.param_names[x_dim])\n",
        "            plt.ylabel('Value')\n",
        "\n",
        "            # Plot the sampled points\n",
        "            for iP in range(len(self.x) if step is None else step):\n",
        "                plt.plot(self.x[iP][x_dim], self.y[iP], '.', color='0.6')\n",
        "\n",
        "        # 2D plot\n",
        "        else:\n",
        "\n",
        "            # Predict y as a fn of x (other params being @ middle of bounds)\n",
        "            x_pred = np.ones((res*res, self.num_dims))\n",
        "            x_pred *= (np.array(self.bounds)\n",
        "                       .mean(axis=1)\n",
        "                       .reshape(-1, self.num_dims))\n",
        "            xp, yp = np.meshgrid(\n",
        "                np.linspace(self.lb[x_dim], self.ub[x_dim], res),\n",
        "                np.linspace(self.lb[y_dim], self.ub[y_dim], res))\n",
        "            x_pred[:,x_dim] = xp.reshape(-1)\n",
        "            x_pred[:,y_dim] = yp.reshape(-1)\n",
        "            y_pred = self._pred_gp(x_pred, return_std=False)\n",
        "\n",
        "            # Plot the Gaussian process\n",
        "            plt.imshow(y_pred.reshape((res, res)), aspect='auto', \n",
        "                       interpolation='bicubic', origin='lower',\n",
        "                       extent=(self.lb[x_dim], self.ub[x_dim], \n",
        "                               self.lb[y_dim], self.ub[y_dim]))\n",
        "            plt.colorbar()\n",
        "\n",
        "            # Plot the sampled points\n",
        "            for iP in range(len(self.x) if step is None else step):\n",
        "                plt.plot(self.x[iP][x_dim], self.x[iP][y_dim], \n",
        "                         '.', color='0.6')\n",
        "\n",
        "\n",
        "    def plot_ei_surface(self, x_dim=None, y_dim=None, res=100, step=None,\n",
        "                        refit=True):\n",
        "        \"\"\"Plot the expected improvement surface.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_dim : str or int\n",
        "            Name or index of parameter to plot on the x axis.\n",
        "            If none specified, uses the first parameter.\n",
        "        y_dim : str or int\n",
        "            Name or index of parameter to plot on the y axis.\n",
        "            If none specified, plots only a 1D plot of x_dim\n",
        "        res : int > 1\n",
        "            Resolution of the plot\n",
        "        step : None or int\n",
        "            Plot loss surface only from points up to step step.\n",
        "        refit : bool\n",
        "            Whether to re-fit the Gaussian process.\n",
        "            Default = True\n",
        "        \"\"\"\n",
        "\n",
        "        # Check inputs\n",
        "        if x_dim is not None and not isinstance(x_dim, (int, str)):\n",
        "            raise TypeError('x_dim must be None, str, or int')\n",
        "        if y_dim is not None and not isinstance(y_dim, (int, str)):\n",
        "            raise TypeError('y_dim must be None, str, or int')\n",
        "        if not isinstance(res, int):\n",
        "            raise TypeError('res must be an int')\n",
        "        if res < 1:\n",
        "            raise ValueError('res must be positive')\n",
        "        if step is not None and not isinstance(step, int):\n",
        "            raise TypeError('step must be None or an int')\n",
        "        if isinstance(step, int) and step < 0:\n",
        "            raise ValueError('step must be non-negative')\n",
        "\n",
        "        # Convert x_dim and y_dim to int if they are strings\n",
        "        if isinstance(x_dim, str):\n",
        "            x_dim = self.param_names.index(x_dim)\n",
        "        if isinstance(y_dim, str):\n",
        "            y_dim = self.param_names.index(y_dim)\n",
        "\n",
        "        # Set x_dim if not specified\n",
        "        if x_dim is None:\n",
        "            x_dim = 0\n",
        "\n",
        "        # Fit GP to 1st ``step`` steps\n",
        "        if refit:\n",
        "            self._fit_gp(step=step)\n",
        "\n",
        "        # 1D plot\n",
        "        if y_dim is None:\n",
        "\n",
        "            # Predict y as a fn of x (other params being @ middle of bounds)\n",
        "            x_pred = np.ones((res, self.num_dims))\n",
        "            x_pred *= (np.array(self.bounds)\n",
        "                       .mean(axis=1)\n",
        "                       .reshape(-1, self.num_dims))\n",
        "            x_pred[:,x_dim] = np.linspace(self.lb[x_dim], self.ub[x_dim], res)\n",
        "            ei = self._expected_improvement(x_pred)\n",
        "\n",
        "            # Plot the expected improvement\n",
        "            plt.plot(x_pred[:, x_dim], ei)\n",
        "            plt.xlabel(self.param_names[x_dim])\n",
        "            plt.ylabel('Expected Improvement')\n",
        "\n",
        "        # 2D plot\n",
        "        else:\n",
        "            pass\n",
        "            # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SM_h3SfMZycp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization\n",
        "\n",
        "TODO: using the gaussian process optimizer to optimize hyperparameters of machine learning models"
      ]
    },
    {
      "metadata": {
        "id": "AfVW08JWZy5f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize_params(X, y, model, bounds,\n",
        "                    metric=make_scorer(mean_squared_error),\n",
        "                    minimize=True,\n",
        "                    n_splits=3,\n",
        "                    shuffle=True,\n",
        "                    max_time=None,\n",
        "                    max_evals=50,\n",
        "                    n_random=5):\n",
        "    \"\"\"Optimize model parameters using cross-fold validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : pandas DataFrame\n",
        "        Independent variable values (features)\n",
        "    y : pandas Series\n",
        "        Dependent variable values (target)\n",
        "    model : sklearn Estimator\n",
        "        Predictive model to optimize\n",
        "    bounds : dict\n",
        "        Parameter bounds.\n",
        "    n_splits : int\n",
        "        Number of cross-validation folds.\n",
        "    shuffle : bool\n",
        "        Whether to shuffle samples before splitting into CV folds.\n",
        "    max_time : None or float\n",
        "        Give up after this many seconds\n",
        "    max_evals : int\n",
        "        Max number of cross-validation evaluations to perform.\n",
        "    n_random : int\n",
        "        Number of evaluations to use random parameter combinations\n",
        "        before switching to Bayesian global optimization.\n",
        "    metric : str or sklearn scorer\n",
        "        What metric to use for evaluation.  One of:\n",
        "\n",
        "        * 'r2' - coefficient of determination (maximize)\n",
        "        * 'mse' - mean squared error (minimize)\n",
        "        * 'mae' - mean absolute error (minimize)\n",
        "        * 'accuracy' or 'acc' - accuracy (maximize)\n",
        "        * 'auc' - area under the ROC curve (maximize)\n",
        "\n",
        "    minimize : bool\n",
        "        Whether to minimize ``metric``.  \n",
        "        If true, minimize; if false, maximize; if None, \n",
        "        use the default for the metric.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    opt_params : dict\n",
        "        Optimal parameters.  Dict of the same format as bounds,\n",
        "        except instead of tuples, the values contain the optimal\n",
        "        parameter values.\n",
        "    optimizer : dsutils.optimization.GaussianProcessOptimizer\n",
        "        Optimizer used to select the points.  Contains the history\n",
        "        of all points which were sampled.\n",
        "    \"\"\"\n",
        "\n",
        "    # Collect info about parameters to optimize\n",
        "    Np = len(bounds) #number of parameters\n",
        "    step_params = [e for e in bounds]\n",
        "    steps = [e.split('__')[0] for e in step_params]\n",
        "    params = [e.split('__')[1] for e in step_params]\n",
        "    lb = [bounds[e][0] for e in step_params]\n",
        "    ub = [bounds[e][1] for e in step_params]\n",
        "    dtypes = [bounds[e][2] for e in step_params]\n",
        "\n",
        "    # Initialize the Gaussian process optimizer\n",
        "    gpo = GaussianProcessOptimizer(lb, ub, dtypes, step_params,\n",
        "                                   minimize=minimize)\n",
        "\n",
        "    # Create a cross-fold generator\n",
        "    kf = KFold(n_splits=n_splits, shuffle=shuffle)\n",
        "\n",
        "    # Search for optimal parameters\n",
        "    start_time = time.time()\n",
        "    for i in range(max_evals):\n",
        "\n",
        "        # Give up if we've spent too much time\n",
        "        if max_time is not None and time.time()-start_time > max_time:\n",
        "            break\n",
        "\n",
        "        # Get next set of parameters to try\n",
        "        if i < n_random:\n",
        "            new_params = gpo.random_point()\n",
        "        else:\n",
        "            new_params = gpo.next_point()\n",
        "\n",
        "        # Modify model to use new parameters\n",
        "        for iP in range(Np):\n",
        "            tP = {params[iP]: new_params[iP]}\n",
        "            model.named_steps[steps[iP]].set_params(**tP)\n",
        "\n",
        "        # Compute and store cross-validated metric\n",
        "        scores = cross_val_score(model, X, y, cv=kf,\n",
        "                                 scoring=scorer, n_jobs=n_jobs)\n",
        "\n",
        "        # Store parameters and scores\n",
        "        gpo.add_point(new_params, scores)\n",
        "\n",
        "    # Return optimal parameters and the optimizer object used\n",
        "    opt_params = gpo.best_point(get_dict=True)\n",
        "    return opt_params, gpo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wlj88aKVaCD3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimizing a Hyperparameter\n",
        "\n",
        "TODO: test on some real data, optimize a hyperparam in 1D, plot score vs hyperparam value"
      ]
    },
    {
      "metadata": {
        "id": "F6QVpaHz3fLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjAlpXVT27u8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimizing Multiple Hyperparameters\n",
        "\n",
        "TODO: test on some real data, optimize the hyperparams in 2D, plot animation for 2D case"
      ]
    },
    {
      "metadata": {
        "id": "2t1vDO9Q3Sd1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}